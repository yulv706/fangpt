#!/usr/bin/env python3
"""
ç»Ÿä¸€åˆ†å­ç”Ÿæˆè„šæœ¬
è‡ªåŠ¨æ£€æµ‹æ¨¡å‹é…ç½®ï¼Œæ”¯æŒæ— æ¡ä»¶ã€å±æ€§æ¡ä»¶ã€è„šæ‰‹æ¶æ¡ä»¶å’Œæ··åˆæ¡ä»¶ç”Ÿæˆ
"""

from utils import check_novelty, sample, canonic_smiles
from dataset import SmileDataset
from rdkit.Chem import QED
from rdkit.Chem import Crippen
from rdkit.Chem.Descriptors import ExactMolWt
from rdkit import Chem
import math
from tqdm import tqdm
import argparse
from model import GPT, GPTConfig
import pandas as pd
import torch
import numpy as np
import matplotlib.pyplot as plt
import re
import moses
import json
import warnings
import os
import sys

# æŠ‘åˆ¶RDKitçš„å¼ƒç”¨è­¦å‘Š
warnings.filterwarnings("ignore", message=".*DEPRECATION WARNING.*")
from rdkit.Chem import RDConfig
from rdkit import Chem

# å¯¼å…¥SA Score
sys.path.append(os.path.join(RDConfig.RDContribDir, 'SA_Score'))
import sascorer
from rdkit.Chem.rdMolDescriptors import CalcTPSA

def get_mol(smiles_or_mol):
    """å°†SMILESå­—ç¬¦ä¸²è½¬æ¢ä¸ºRDKitåˆ†å­å¯¹è±¡"""
    if isinstance(smiles_or_mol, str):
        if len(smiles_or_mol) == 0:
            return None
        mol = Chem.MolFromSmiles(smiles_or_mol)
        if mol is None:
            return None
        try:
            Chem.SanitizeMol(mol)
        except ValueError:
            return None
        return mol
    return smiles_or_mol


def normalize_symbol(symbol):
    if symbol is None:
        return ''
    symbol = str(symbol).strip().replace('[', '').replace(']', '')
    if not symbol:
        return ''
    if len(symbol) == 1:
        return symbol.upper()
    return symbol[0].upper() + symbol[1:].lower()


def dedupe_preserve_order(items):
    seen = set()
    ordered = []
    for item in items:
        if item and item not in seen:
            ordered.append(item)
            seen.add(item)
    return ordered


def parse_atom_condition(atom_symbols, condition_args, on_value=1.0, off_value=0.0):
    if not atom_symbols:
        return None
    if not condition_args:
        return [on_value] * len(atom_symbols)
    try:
        values = [float(v) for v in condition_args]
        if len(values) != len(atom_symbols):
            raise ValueError
        return values
    except ValueError:
        normalized_targets = {normalize_symbol(v) for v in condition_args if normalize_symbol(v)}
        if not normalized_targets or normalized_targets == {'None'}:
            return [off_value] * len(atom_symbols)
        return [on_value if sym in normalized_targets else off_value for sym in atom_symbols]

def detect_model_config(weight_path):
    """è‡ªåŠ¨æ£€æµ‹æ¨¡å‹é…ç½®"""
    print("ğŸ” æ­£åœ¨è‡ªåŠ¨æ£€æµ‹æ¨¡å‹é…ç½®...")
    
    checkpoint_data = torch.load(weight_path, map_location='cpu', weights_only=False)
    
    # ğŸ”§ æ£€æŸ¥æ˜¯å¦ä¸ºæ–°æ ¼å¼ï¼ˆåŒ…å«è®­ç»ƒé…ç½®ï¼‰
    if isinstance(checkpoint_data, dict) and 'model_state_dict' in checkpoint_data:
        print("ğŸ†• æ£€æµ‹åˆ°æ–°æ ¼å¼æƒé‡æ–‡ä»¶ï¼ˆåŒ…å«è®­ç»ƒé…ç½®ï¼‰")
        checkpoint = checkpoint_data['model_state_dict']
        training_config = checkpoint_data.get('training_config', {})
        
        # ç›´æ¥ä½¿ç”¨ä¿å­˜çš„è®­ç»ƒé…ç½®
        if training_config:
            print("âœ… ä½¿ç”¨ä¿å­˜çš„è®­ç»ƒé…ç½®ä¿¡æ¯")
            # ğŸ”§ ä¿®å¤ï¼šåªæœ‰å½“num_props > 0æ—¶æ‰æ˜¾ç¤ºpropsï¼Œé¿å…æ··æ·†
            num_props = training_config.get('num_props', 0)
            props_list = training_config.get('props', []) if num_props > 0 else []
            
            atom_cond = training_config.get('atom_cond', False)
            atom_list = training_config.get('atom_list', []) if atom_cond else []
            atom_vocab = training_config.get('atom_vocab_size', len(atom_list)) if atom_cond else 0
            config = {
                'vocab_size': training_config.get('vocab_size', 26),
                'n_embd': training_config.get('n_embd', 256),
                'block_size': training_config.get('block_size', 54),
                'n_layer': training_config.get('n_layer', 8),
                'n_head': training_config.get('n_head', 8),
                'num_props': num_props,
                'has_props': num_props > 0,
                'scaffold_maxlen': training_config.get('scaffold_maxlen', 0),
                'uses_scaffold': training_config.get('scaffold', False),
                'has_lstm': training_config.get('lstm', False),
                'lstm_layers': training_config.get('lstm_layers', 2),
                'props': props_list,
                'data_name': training_config.get('data_name', 'moses2'),
                'atom_cond': atom_cond,
                'atom_list': atom_list,
                'atom_vocab_size': atom_vocab,
                'mask_size': training_config.get('block_size', 54) + 
                           int(num_props > 0) + 
                           training_config.get('scaffold_maxlen', 0) + 
                           (1 if atom_cond else 0)
            }
            return config
    else:
        print("ğŸ”„ æ£€æµ‹åˆ°æ—§æ ¼å¼æƒé‡æ–‡ä»¶ï¼ˆä»…åŒ…å«æ¨¡å‹æƒé‡ï¼‰")
        checkpoint = checkpoint_data
    
    # æ—§æ ¼å¼çš„å¯å‘å¼æ£€æµ‹é€»è¾‘
    # åŸºæœ¬å‚æ•°
    vocab_size = checkpoint['tok_emb.weight'].shape[0]
    n_embd = checkpoint['tok_emb.weight'].shape[1]
    block_size = checkpoint['pos_emb'].shape[1]
    
    # è·å–æ©ç å¤§å°
    mask_keys = [k for k in checkpoint.keys() if 'attn.mask' in k]
    if mask_keys:
        mask_size = checkpoint[mask_keys[0]].shape[-1]
    else:
        mask_size = block_size
    
    # å±‚æ•°
    layer_keys = [k for k in checkpoint.keys() if k.startswith('blocks.')]
    layer_nums = set()
    for key in layer_keys:
        parts = key.split('.')
        if len(parts) >= 2 and parts[1].isdigit():
            layer_nums.add(int(parts[1]))
    n_layer = max(layer_nums) + 1 if layer_nums else 8
    
    # æ³¨æ„åŠ›å¤´æ•°ï¼ˆç®€åŒ–æ¨æ–­ï¼‰
    n_head = 8 if n_embd % 8 == 0 else 1
    
    # æ£€æŸ¥æ¡ä»¶ç”Ÿæˆç‰¹å¾
    has_prop_nn = any('prop_nn' in k for k in checkpoint.keys())
    has_lstm = any('lstm' in k.lower() for k in checkpoint.keys())
    
    # åˆ†ææ©ç é…ç½®ï¼šmask_size = block_size + int(bool(num_props)) + scaffold_maxlen
    extra_size = mask_size - block_size
    
    if has_prop_nn:
        num_props = 1  # ç®€åŒ–å‡è®¾å•å±æ€§
        scaffold_maxlen = extra_size - 1
    else:
        num_props = 0
        scaffold_maxlen = extra_size
    
    # ç¡®ä¿scaffold_maxlenåˆç†
    if scaffold_maxlen < 0:
        scaffold_maxlen = 0
    
    config = {
        'vocab_size': vocab_size,
        'n_embd': n_embd,
        'block_size': block_size,
        'n_layer': n_layer,
        'n_head': n_head,
        'num_props': num_props,
        'has_props': has_prop_nn,
        'scaffold_maxlen': scaffold_maxlen,
        'uses_scaffold': scaffold_maxlen > 0,
        'has_lstm': has_lstm,
        'mask_size': mask_size,
        'props': [],  # æ—§æ ¼å¼æ— æ³•æ¨æ–­å…·ä½“å±æ€§
        'data_name': 'moses2' if vocab_size == 26 else 'guacamol2',
        'atom_cond': False,
        'atom_list': [],
        'atom_vocab_size': 0
    }

    return config

def create_dummy_scaffold(scaffold_maxlen, stoi):
    """åˆ›å»ºå¡«å……çš„è™šæ‹Ÿè„šæ‰‹æ¶"""
    if scaffold_maxlen <= 0:
        return None
    dummy_scaffold = '<' * scaffold_maxlen
    pattern = "(\[[^\]]+]|<|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\(|\)|\.|=|#|-|\+|\\\\|\/|:|~|@|\?|>|\*|\$|\%[0-9]{2}|[0-9])"
    regex = re.compile(pattern)
    return torch.tensor([stoi[s] for s in regex.findall(dummy_scaffold)], dtype=torch.long)

def generate_molecules(model, stoi, itos, block_size, batch_size, gen_iter, 
                      prop_tensor=None, scaffold_tensor=None, atom_tensor=None, context="C"):
    """ç”Ÿæˆåˆ†å­çš„æ ¸å¿ƒå‡½æ•°"""
    pattern = "(\[[^\]]+]|<|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\(|\)|\.|=|#|-|\+|\\\\|\/|:|~|@|\?|>|\*|\$|\%[0-9]{2}|[0-9])"
    regex = re.compile(pattern)
    
    molecules = []
    
    for i in tqdm(range(gen_iter), desc="ç”Ÿæˆä¸­"):
        x = torch.tensor([stoi[s] for s in regex.findall(context)], dtype=torch.long)[None, ...].repeat(batch_size, 1).to('cuda')
        
        # å‡†å¤‡å±æ€§å’Œè„šæ‰‹æ¶è¾“å…¥
        p = prop_tensor.repeat(batch_size, 1).to('cuda') if prop_tensor is not None else None
        sca = scaffold_tensor[None, ...].repeat(batch_size, 1).to('cuda') if scaffold_tensor is not None else None
        
        # æ¸…ç†GPUç¼“å­˜
        torch.cuda.empty_cache()
        
        # ç”Ÿæˆ
        atom = None
        if atom_tensor is not None:
            atom = atom_tensor[None, ...].repeat(batch_size, 1).to('cuda')

        with torch.no_grad():
            y = sample(model, x, block_size, temperature=1, sample=True, top_k=None, prop=p, scaffold=sca, atom_cond=atom)
        
        # è§£ç åˆ†å­
        for gen_mol in y:
            completion = ''.join([itos[int(i)] for i in gen_mol])
            completion = completion.replace('<', '')
            mol = get_mol(completion)
            if mol:
                molecules.append(mol)
    
    return molecules

def calculate_metrics(molecules, results_df, data, data_name, batch_size, gen_iter):
    """è®¡ç®—ç”ŸæˆæŒ‡æ ‡"""
    # è®¡ç®—å»é‡å’Œæ–°é¢–æ€§
    canon_smiles = [canonic_smiles(s) for s in results_df['smiles']]
    unique_smiles = list(set(canon_smiles))
    
    if 'moses' in data_name:
        novel_ratio = check_novelty(unique_smiles, set(data[data['split'] == 'train']['smiles']))
    else:
        novel_ratio = check_novelty(unique_smiles, set(data[data['source'] == 'train']['smiles']))
    
    # è®¡ç®—åˆ†å­æ€§è´¨
    results_df['qed'] = results_df['molecule'].apply(lambda x: QED.qed(x))
    results_df['sas'] = results_df['molecule'].apply(lambda x: sascorer.calculateScore(x))
    results_df['logp'] = results_df['molecule'].apply(lambda x: Crippen.MolLogP(x))
    results_df['tpsa'] = results_df['molecule'].apply(lambda x: CalcTPSA(x))
    
    # è®¡ç®—æ¯”ç‡
    validity = len(results_df) / (batch_size * gen_iter)
    uniqueness = len(unique_smiles) / len(results_df) if len(results_df) > 0 else 0
    novelty = novel_ratio / 100
    
    results_df['validity'] = np.round(validity, 3)
    results_df['unique'] = np.round(uniqueness, 3)
    results_df['novelty'] = np.round(novelty, 3)
    
    return results_df, validity, uniqueness, novelty

def main():
    parser = argparse.ArgumentParser(description='ç»Ÿä¸€åˆ†å­ç”Ÿæˆè„šæœ¬')
    
    # å¿…éœ€å‚æ•°
    parser.add_argument('--model_weight', type=str, required=True, help='æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--csv_name', type=str, required=True, help='è¾“å‡ºCSVæ–‡ä»¶å')
    
    # ç”Ÿæˆå‚æ•°
    parser.add_argument('--data_name', type=str, default='moses2', help='æ•°æ®é›†åç§°')
    parser.add_argument('--batch_size', type=int, default=512, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--gen_size', type=int, default=10000, help='ç”Ÿæˆåˆ†å­æ€»æ•°')
    
    # æ¡ä»¶ç”Ÿæˆå‚æ•°
    parser.add_argument('--props', nargs='+', default=[], help='å±æ€§æ¡ä»¶')
    parser.add_argument('--scaffold', action='store_true', help='å¯ç”¨è„šæ‰‹æ¶æ¡ä»¶ç”Ÿæˆ')
    parser.add_argument('--lstm', action='store_true', help='ä½¿ç”¨LSTMå¤„ç†è„šæ‰‹æ¶')
    parser.add_argument('--lstm_layers', type=int, default=2, help='LSTMå±‚æ•°')
    parser.add_argument('--atom_list', nargs='+', default=None, help='è®­ç»ƒæ—¶ä½¿ç”¨çš„åŸå­åˆ—è¡¨ï¼ˆæŒ‰é¡ºåºï¼‰')
    parser.add_argument('--atom_condition', nargs='+', default=None, help='åŸå­æ¡ä»¶å‘é‡æˆ–éœ€è¦æ¿€æ´»çš„åŸå­ç¬¦å·')
    parser.add_argument('--atom_on_value', type=float, default=1.0, help='æ¿€æ´»åŸå­çš„å–å€¼')
    parser.add_argument('--atom_off_value', type=float, default=0.0, help='æœªæ¿€æ´»åŸå­çš„å–å€¼')
    
    # å¯é€‰çš„æ‰‹åŠ¨é…ç½®å‚æ•°
    parser.add_argument('--vocab_size', type=int, default=None, help='è¯æ±‡è¡¨å¤§å°ï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰')
    parser.add_argument('--block_size', type=int, default=None, help='åºåˆ—é•¿åº¦ï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰')
    parser.add_argument('--n_layer', type=int, default=None, help='å±‚æ•°ï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰')
    parser.add_argument('--n_head', type=int, default=None, help='æ³¨æ„åŠ›å¤´æ•°ï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰')
    parser.add_argument('--n_embd', type=int, default=None, help='åµŒå…¥ç»´åº¦ï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰')
    
    args = parser.parse_args()

    # è‡ªåŠ¨æ£€æµ‹æ¨¡å‹é…ç½®
    detected_config = detect_model_config(args.model_weight)
    
    print("ğŸ“‹ æ£€æµ‹åˆ°çš„æ¨¡å‹é…ç½®:")
    for key, value in detected_config.items():
        print(f"  {key}: {value}")
    
    # ä½¿ç”¨æ£€æµ‹åˆ°çš„é…ç½®ï¼Œå…è®¸å‘½ä»¤è¡Œå‚æ•°è¦†ç›–
    vocab_size = args.vocab_size or detected_config['vocab_size']
    block_size = args.block_size or detected_config['block_size']
    n_layer = args.n_layer or detected_config['n_layer']
    n_head = args.n_head or detected_config['n_head']
    n_embd = args.n_embd or detected_config['n_embd']

    detected_atom_list = detected_config.get('atom_list', [])
    atom_candidates = args.atom_list if args.atom_list is not None else detected_atom_list
    atom_symbols = dedupe_preserve_order([normalize_symbol(a) for a in atom_candidates or [] if normalize_symbol(a)])
    atom_cond_enabled = detected_config.get('atom_cond', bool(atom_symbols))
    if atom_cond_enabled and not atom_symbols and detected_atom_list:
        atom_symbols = dedupe_preserve_order([normalize_symbol(a) for a in detected_atom_list if normalize_symbol(a)])
        atom_cond_enabled = bool(atom_symbols)
    elif not atom_symbols:
        atom_cond_enabled = False

    atom_condition_values = parse_atom_condition(
        atom_symbols,
        args.atom_condition,
        on_value=args.atom_on_value,
        off_value=args.atom_off_value
    ) if atom_cond_enabled else None
    atom_condition_tensor = torch.tensor(atom_condition_values, dtype=torch.float) if atom_condition_values is not None else None
    
    # åŠ è½½æ•°æ®å’Œè¯æ±‡è¡¨
    data = pd.read_csv(f'datasets/{args.data_name}.csv')
    data = data.dropna(axis=0).reset_index(drop=True)
    data.columns = data.columns.str.lower()
    
    # æ ¹æ®æ£€æµ‹åˆ°çš„vocab_sizeé€‰æ‹©æ­£ç¡®çš„è¯æ±‡è¡¨
    detected_vocab_size = detected_config['vocab_size']
    vocab_file_candidates = [
        f'{args.data_name}_stoi.json',
        'guacamol2_stoi.json',
        'moses2_stoi.json'
    ]
    
    stoi = None
    for vocab_file in vocab_file_candidates:
        if os.path.exists(vocab_file):
            test_stoi = json.load(open(vocab_file, 'r'))
            if len(test_stoi) == detected_vocab_size:
                stoi = test_stoi
                print(f"âœ… æ‰¾åˆ°åŒ¹é…çš„è¯æ±‡è¡¨: {vocab_file} (å¤§å°: {len(stoi)})")
                break
    
    if stoi is None:
        print(f"âŒ è­¦å‘Šï¼šæ— æ³•æ‰¾åˆ°åŒ¹é…çš„è¯æ±‡è¡¨ï¼Œä½¿ç”¨é»˜è®¤çš„ {args.data_name}_stoi.json")
        stoi = json.load(open(f'{args.data_name}_stoi.json', 'r'))
    
    itos = {i: ch for ch, i in stoi.items()}
    
    print(f"\nğŸ“– æ•°æ®é›†ä¿¡æ¯:")
    print(f"  æ•°æ®é›†: {args.data_name}")
    print(f"  è¯æ±‡è¡¨å¤§å°: {len(itos)}")
    if atom_cond_enabled:
        print(f"  åŸå­æ¡ä»¶: {atom_symbols}")
        if atom_condition_values is not None:
            print(f"  åŸå­å‘é‡: {atom_condition_values}")
    else:
        print("  åŸå­æ¡ä»¶: æœªå¯ç”¨")
    
    # ğŸ”§ æ–°å¢ï¼šå¤„ç†å±æ€§æ¡ä»¶è‡ªåŠ¨æ£€æµ‹
    # å‡†å¤‡å±æ€§æ¡ä»¶å€¼
    if 'guacamol' in args.data_name:
        single_prop_values = {
            'qed': [0.3, 0.5, 0.7], 
            'sas': [2.0, 3.0, 4.0], 
            'logp': [2.0, 4.0, 6.0], 
            'tpsa': [40.0, 80.0, 120.0]
        }
    else:
        single_prop_values = {
            'qed': [0.6, 0.725, 0.85], 
            'sas': [2.0, 2.75, 3.5], 
            'logp': [1.0, 2.0, 3.0], 
            'tpsa': [30.0, 60.0, 90.0]
        }
    
    def create_prop_conditions(props_list):
        """åˆ›å»ºå±æ€§æ¡ä»¶åˆ—è¡¨"""
        if len(props_list) == 1:
            # å•å±æ€§ï¼šç›´æ¥ä½¿ç”¨æ•°å€¼åˆ—è¡¨
            return single_prop_values.get(props_list[0], [0.5])
        else:
            # å¤šå±æ€§ï¼šç”Ÿæˆç»„åˆå‘é‡
            prop_values_lists = []
            for prop in props_list:
                prop_values_lists.append(single_prop_values.get(prop, [0.5]))
            
            # å–æ¯ä¸ªå±æ€§çš„å¯¹åº”å€¼æ„æˆå‘é‡
            prop_conditions = []
            for i in range(min(len(lst) for lst in prop_values_lists)):
                prop_vector = [lst[i] for lst in prop_values_lists]
                prop_conditions.append(prop_vector)
            return prop_conditions
    
    # ç¡®å®šç”Ÿæˆæ¡ä»¶
    prop_conditions = None
    if args.props:
        # ç”¨æˆ·æŒ‡å®šäº†å±æ€§
        prop_conditions = create_prop_conditions(args.props)
    elif detected_config.get('props') and detected_config.get('num_props', 0) > 0:
        # ä»æ¨¡å‹é…ç½®ä¸­æ£€æµ‹åˆ°å±æ€§ï¼ˆæ–°æ ¼å¼ï¼‰ï¼Œå¹¶ä¸”æ¨¡å‹ç¡®å®æœ‰å±æ€§æ”¯æŒ
        detected_props = detected_config['props']
        print(f"ğŸ” ä»æ¨¡å‹é…ç½®ä¸­æ£€æµ‹åˆ°å±æ€§: {detected_props}")
        prop_conditions = create_prop_conditions(detected_props)
        args.props = detected_props  # æ›´æ–°argsä»¥ä¾¿åç»­ä½¿ç”¨
        print(f"âœ… è‡ªåŠ¨è®¾ç½®å±æ€§æ¡ä»¶: {detected_props}")
    elif detected_config.get('num_props', 0) == 0:
        # æ— æ¡ä»¶æ¨¡å‹ï¼Œä¸è®¾ç½®ä»»ä½•å±æ€§æ¡ä»¶
        print("ğŸ” æ£€æµ‹åˆ°æ— æ¡ä»¶æ¨¡å‹ï¼Œä¸ä½¿ç”¨å±æ€§æ¡ä»¶")
        args.props = []  # ç¡®ä¿ä¸ºç©ºåˆ—è¡¨
    
    # é…ç½®æ¨¡å‹
    num_props = len(args.props) if args.props else 0
    model_uses_scaffold = detected_config['uses_scaffold']
    scaffold_maxlen = detected_config['scaffold_maxlen']
    
    # å¦‚æœç”¨æˆ·æ˜ç¡®è¦æ±‚ä½¿ç”¨è„šæ‰‹æ¶ä½†æ¨¡å‹ä¸æ”¯æŒï¼Œç»™å‡ºè­¦å‘Š
    if args.scaffold and not model_uses_scaffold:
        print("âš ï¸  è­¦å‘Šï¼šç”¨æˆ·è¦æ±‚è„šæ‰‹æ¶ç”Ÿæˆï¼Œä½†æ¨¡å‹æœªé…ç½®è„šæ‰‹æ¶æ”¯æŒ")
        print("   å°†å°è¯•ä½¿ç”¨æ— è„šæ‰‹æ¶æ¨¡å¼è¿›è¡Œç”Ÿæˆ")
        args.scaffold = False
    
    print(f"\nğŸ¯ ç”Ÿæˆé…ç½®:")
    print(f"  å±æ€§æ¡ä»¶: {args.props if args.props else 'æ— '}")
    print(f"  è„šæ‰‹æ¶æ¡ä»¶: {'æ˜¯' if args.scaffold else 'å¦'}")
    print(f"  æ¨¡å‹è„šæ‰‹æ¶æ”¯æŒ: {'æ˜¯' if model_uses_scaffold else 'å¦'}")
    print(f"  è„šæ‰‹æ¶æœ€å¤§é•¿åº¦: {scaffold_maxlen}")
    
    # åˆ›å»ºæ¨¡å‹
    mconf = GPTConfig(vocab_size, block_size, num_props=num_props,
                     n_layer=n_layer, n_head=n_head, n_embd=n_embd,
                     scaffold=model_uses_scaffold, scaffold_maxlen=scaffold_maxlen,
                     lstm=args.lstm, lstm_layers=args.lstm_layers,
                     atom_cond=atom_cond_enabled and bool(atom_symbols), atom_vocab_size=len(atom_symbols))
    model = GPT(mconf)
    
    # åŠ è½½æƒé‡
    checkpoint_data = torch.load(args.model_weight, map_location='cpu', weights_only=False)
    if isinstance(checkpoint_data, dict) and 'model_state_dict' in checkpoint_data:
        # æ–°æ ¼å¼ï¼šåŒ…å«è®­ç»ƒé…ç½®
        model_state_dict = checkpoint_data['model_state_dict']
    else:
        # æ—§æ ¼å¼ï¼šç›´æ¥æ˜¯state_dict
        model_state_dict = checkpoint_data
    
    model.load_state_dict(model_state_dict)
    model.to('cuda')
    print('âœ… æ¨¡å‹åŠ è½½æˆåŠŸ')
    
    # å‡†å¤‡ç”Ÿæˆå‚æ•°
    gen_iter = math.ceil(args.gen_size / args.batch_size)
    context = "C"
    pattern = "(\[[^\]]+]|<|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\(|\)|\.|=|#|-|\+|\\\\|\/|:|~|@|\?|>|\*|\$|\%[0-9]{2}|[0-9])"
    regex = re.compile(pattern)
    
    # ğŸ”§ æ³¨æ„ï¼šå±æ€§æ¡ä»¶å·²åœ¨ä¸Šé¢çš„è‡ªåŠ¨æ£€æµ‹é€»è¾‘ä¸­å¤„ç†
    
    scaffold_conditions = None
    if args.scaffold:
        # é¢„å®šä¹‰çš„è„šæ‰‹æ¶æ¡ä»¶
        base_scaffolds = [
            'O=C(Cc1ccccc1)NCc1ccccc1', 
            'c1cnc2[nH]ccc2c1', 
            'c1ccc(-c2ccnnc2)cc1', 
            'c1ccc(-n2cnc3ccccc32)cc1', 
            'O=C(c1cc[nH]c1)N1CCN(c2ccccc2)CC1'
        ]
        scaffold_conditions = [s + '<' * (scaffold_maxlen - len(regex.findall(s))) for s in base_scaffolds]
    
    # ç¡®å®šç”Ÿæˆæ¨¡å¼
    if not prop_conditions and not scaffold_conditions:
        generation_mode = "æ— æ¡ä»¶ç”Ÿæˆ"
    elif prop_conditions and not scaffold_conditions:
        generation_mode = f"å±æ€§æ¡ä»¶ç”Ÿæˆ ({','.join(args.props)})"
    elif not prop_conditions and scaffold_conditions:
        generation_mode = "è„šæ‰‹æ¶æ¡ä»¶ç”Ÿæˆ"
    else:
        generation_mode = f"æ··åˆæ¡ä»¶ç”Ÿæˆ ({','.join(args.props)} + è„šæ‰‹æ¶)"
    
    print(f"\nğŸš€ å¼€å§‹{generation_mode}...")
    
    # æ‰§è¡Œç”Ÿæˆ
    all_results = []
    
    # ç¡®å®šå¾ªç¯æ¡ä»¶
    prop_loop = prop_conditions or [None]
    scaffold_loop = scaffold_conditions or [None]
    
    for prop_cond in prop_loop:
        for scaffold_cond in scaffold_loop:
            # å‡†å¤‡æ¡ä»¶æè¿°
            cond_desc = []
            if prop_cond is not None:
                if len(args.props) == 1:
                    cond_desc.append(f"å±æ€§={prop_cond}")
                else:
                    # å¤šå±æ€§ï¼šæ˜¾ç¤ºå±æ€§åå’Œå¯¹åº”å€¼
                    prop_str = ", ".join([f"{prop}={val}" for prop, val in zip(args.props, prop_cond)])
                    cond_desc.append(f"å±æ€§=({prop_str})")
            if scaffold_cond is not None:
                cond_desc.append(f"è„šæ‰‹æ¶={scaffold_cond[:20]}...")
            
            desc = ", ".join(cond_desc) if cond_desc else "æ— æ¡ä»¶"
            print(f"\nç”Ÿæˆæ¡ä»¶: {desc}")
            
            # å‡†å¤‡è¾“å…¥å¼ é‡
            prop_tensor = None
            if prop_cond is not None:
                if len(args.props) == 1:
                    prop_tensor = torch.tensor([[prop_cond]])
                else:
                    # å¤šå±æ€§ï¼šprop_cond å·²ç»æ˜¯å‘é‡ï¼Œç›´æ¥æ„é€ å¼ é‡
                    prop_tensor = torch.tensor([prop_cond])
            
            scaffold_tensor = None
            if scaffold_cond is not None:
                scaffold_tensor = torch.tensor([stoi[s] for s in regex.findall(scaffold_cond)], dtype=torch.long)
            elif model_uses_scaffold:
                # æ¨¡å‹éœ€è¦è„šæ‰‹æ¶è¾“å…¥ä½†ç”¨æˆ·æœªæä¾›ï¼Œä½¿ç”¨è™šæ‹Ÿè„šæ‰‹æ¶
                scaffold_tensor = create_dummy_scaffold(scaffold_maxlen, stoi)
            
            # ç”Ÿæˆåˆ†å­
            molecules = generate_molecules(
                model, stoi, itos, block_size, args.batch_size, gen_iter,
                prop_tensor, scaffold_tensor, atom_condition_tensor, context
            )
            
            print(f"æœ‰æ•ˆåˆ†å­æ•°: {len(molecules)}")
            
            # åˆ›å»ºç»“æœDataFrame
            if molecules:
                mol_dict = [{'molecule': mol, 'smiles': Chem.MolToSmiles(mol)} for mol in molecules]
                results_df = pd.DataFrame(mol_dict)
                
                # æ·»åŠ æ¡ä»¶ä¿¡æ¯
                if prop_cond is not None:
                    if len(args.props) == 1:
                        results_df['condition'] = prop_cond
                    else:
                        # å¤šå±æ€§ï¼šä¿å­˜ä¸ºå­—ç¬¦ä¸²æ ¼å¼
                        prop_str = "_".join([f"{prop}={val}" for prop, val in zip(args.props, prop_cond)])
                        results_df['condition'] = prop_str
                
                if scaffold_cond is not None:
                    results_df['scaffold_cond'] = scaffold_cond
                
                # è®¡ç®—æŒ‡æ ‡
                results_df, validity, uniqueness, novelty = calculate_metrics(
                    molecules, results_df, data, args.data_name, args.batch_size, gen_iter
                )
                
                print(f'æœ‰æ•ˆæ€§: {validity:.3f}')
                print(f'å”¯ä¸€æ€§: {uniqueness:.3f}')
                print(f'æ–°é¢–æ€§: {novelty:.3f}')
                
                all_results.append(results_df)
    
    # åˆå¹¶æ‰€æœ‰ç»“æœ
    if all_results:
        final_results = pd.concat(all_results, ignore_index=True)
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        if '/' not in args.csv_name:
            os.makedirs('generated_csvs', exist_ok=True)
            output_path = os.path.join('generated_csvs', args.csv_name)
        else:
            output_path = args.csv_name
        
        # ä¿å­˜ç»“æœ
        final_results.to_csv(f'{output_path}.csv', index=False)
        
        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        canon_smiles = [canonic_smiles(s) for s in final_results['smiles']]
        unique_smiles = list(set(canon_smiles))
        if 'moses' in args.data_name:
            novel_ratio = check_novelty(unique_smiles, set(data[data['split'] == 'train']['smiles']))
        else:
            novel_ratio = check_novelty(unique_smiles, set(data[data['source'] == 'train']['smiles']))
        
        total_expected = args.batch_size * gen_iter * len(prop_loop) * len(scaffold_loop)
        
        print('\n=== ğŸ‰ æœ€ç»ˆç»Ÿè®¡ ===')
        print(f'ç”Ÿæˆæ¨¡å¼: {generation_mode}')
        print(f'æ€»åˆ†å­æ•°: {len(final_results)}')
        print(f'æ€»ä½“æœ‰æ•ˆæ€§: {len(final_results)/total_expected:.3f}')
        print(f'æ€»ä½“å”¯ä¸€æ€§: {len(unique_smiles)/len(final_results):.3f}')
        print(f'æ€»ä½“æ–°é¢–æ€§: {novel_ratio/100:.3f}')
        print(f'ç»“æœå·²ä¿å­˜åˆ°: {output_path}.csv')
    else:
        print("âŒ æœªç”Ÿæˆä»»ä½•æœ‰æ•ˆåˆ†å­")

if __name__ == '__main__':
    main() 
