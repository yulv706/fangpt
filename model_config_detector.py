#!/usr/bin/env python3
"""
ç»Ÿä¸€æ¨¡å‹é…ç½®æ£€æµ‹è„šæœ¬
ç²¾ç¡®æ£€æµ‹PyTorchæ¨¡å‹æƒé‡æ–‡ä»¶(.pt)çš„è®­ç»ƒé…ç½®å¹¶æä¾›ç”Ÿæˆå‘½ä»¤å»ºè®®
"""

import torch
import argparse
import os
import json
from collections import OrderedDict

class ModelConfigDetector:
    """æ¨¡å‹é…ç½®æ£€æµ‹å™¨"""
    
    def __init__(self, weight_path):
        self.weight_path = weight_path
        self.checkpoint = None
        self.config = {}
        
    def load_checkpoint(self):
        """åŠ è½½æƒé‡æ–‡ä»¶"""
        if not os.path.exists(self.weight_path):
            raise FileNotFoundError(f"æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {self.weight_path}")
        
        try:
            checkpoint_data = torch.load(self.weight_path, map_location='cpu', weights_only=False)
            print(f"âœ… æˆåŠŸåŠ è½½æƒé‡æ–‡ä»¶: {self.weight_path}")
            print(f"ğŸ“ æ–‡ä»¶å¤§å°: {os.path.getsize(self.weight_path) / (1024*1024):.2f} MB")
            
            # ğŸ”§ æ£€æŸ¥æ˜¯å¦ä¸ºæ–°æ ¼å¼ï¼ˆåŒ…å«è®­ç»ƒé…ç½®ï¼‰
            if isinstance(checkpoint_data, dict) and 'model_state_dict' in checkpoint_data:
                print("ğŸ†• æ£€æµ‹åˆ°æ–°æ ¼å¼æƒé‡æ–‡ä»¶ï¼ˆåŒ…å«è®­ç»ƒé…ç½®ï¼‰")
                self.checkpoint = checkpoint_data['model_state_dict']
                self.training_config = checkpoint_data.get('training_config', {})
                print(f"ğŸ“‹ è®­ç»ƒé…ç½®ä¿¡æ¯: {self.training_config}")
            else:
                print("ğŸ”„ æ£€æµ‹åˆ°æ—§æ ¼å¼æƒé‡æ–‡ä»¶ï¼ˆä»…åŒ…å«æ¨¡å‹æƒé‡ï¼‰")
                self.checkpoint = checkpoint_data
                self.training_config = {}
            
            print(f"ğŸ”¢ å‚æ•°æ€»æ•°: {len(self.checkpoint)}")
        except Exception as e:
            raise RuntimeError(f"æ— æ³•åŠ è½½æƒé‡æ–‡ä»¶: {str(e)}")
    
    def detect_training_config_from_saved(self):
        """ä»ä¿å­˜çš„è®­ç»ƒé…ç½®ä¸­è·å–ä¿¡æ¯"""
        if hasattr(self, 'training_config') and self.training_config:
            # ç›´æ¥ä»ä¿å­˜çš„é…ç½®ä¸­è¯»å–
            for key, value in self.training_config.items():
                self.config[key] = value
            
            # è®¾ç½®ä¾¿äºæ£€æµ‹çš„æ ‡å¿—
            self.config['has_props'] = self.config.get('num_props', 0) > 0
            self.config['uses_scaffold'] = self.config.get('scaffold', False)
            self.config['has_lstm'] = self.config.get('lstm', False)
            
            return True
        return False
    
    def detect_basic_config(self):
        """æ£€æµ‹åŸºæœ¬é…ç½®å‚æ•°"""
        # è¯æ±‡è¡¨å¤§å°å’ŒåµŒå…¥ç»´åº¦
        if 'tok_emb.weight' in self.checkpoint:
            self.config['vocab_size'] = self.checkpoint['tok_emb.weight'].shape[0]
            self.config['n_embd'] = self.checkpoint['tok_emb.weight'].shape[1]
        
        # åºåˆ—é•¿åº¦
        if 'pos_emb' in self.checkpoint:
            self.config['block_size'] = self.checkpoint['pos_emb'].shape[1]
        
        # å±‚æ•°
        layer_keys = [k for k in self.checkpoint.keys() if k.startswith('blocks.')]
        if layer_keys:
            layer_nums = set()
            for key in layer_keys:
                parts = key.split('.')
                if len(parts) >= 2 and parts[1].isdigit():
                    layer_nums.add(int(parts[1]))
            self.config['n_layer'] = max(layer_nums) + 1 if layer_nums else 0
        
        # æ³¨æ„åŠ›å¤´æ•°æ¨æ–­
        if 'n_embd' in self.config:
            # å¸¸è§çš„å¤´æ•°é…ç½®
            possible_heads = [1, 2, 4, 6, 8, 12, 16, 20, 24, 32]
            for heads in possible_heads:
                if self.config['n_embd'] % heads == 0:
                    self.config['n_head'] = heads
                    break
            if 'n_head' not in self.config:
                self.config['n_head'] = 8  # é»˜è®¤å€¼
        
        # æ³¨æ„åŠ›æ©ç å¤§å°
        mask_keys = [k for k in self.checkpoint.keys() if 'attn.mask' in k]
        if mask_keys:
            self.config['mask_size'] = self.checkpoint[mask_keys[0]].shape[-1]
    
    def detect_conditional_features(self):
        """æ£€æµ‹æ¡ä»¶ç”Ÿæˆç‰¹å¾"""
        # æ£€æµ‹å±æ€§å±‚
        prop_keys = [k for k in self.checkpoint.keys() if 'prop_nn' in k]
        self.config['has_props'] = len(prop_keys) > 0
        
        if self.config['has_props']:
            if 'prop_nn.weight' in self.checkpoint:
                self.config['num_props'] = self.checkpoint['prop_nn.weight'].shape[1]
            else:
                self.config['num_props'] = 1  # é»˜è®¤
        else:
            self.config['num_props'] = 0
        
        # æ£€æµ‹è„šæ‰‹æ¶å±‚
        scaffold_keys = [k for k in self.checkpoint.keys() if 'scaffold' in k.lower()]
        self.config['has_scaffold_layers'] = len(scaffold_keys) > 0
        
        # æ£€æµ‹LSTMå±‚
        lstm_keys = [k for k in self.checkpoint.keys() if 'lstm' in k.lower()]
        self.config['has_lstm'] = len(lstm_keys) > 0
        
        if self.config['has_lstm']:
            # æ¨æ–­LSTMå±‚æ•°
            lstm_layer_keys = [k for k in lstm_keys if 'weight_ih_l' in k or 'weight_hh_l' in k]
            if lstm_layer_keys:
                layer_nums = set()
                for key in lstm_layer_keys:
                    if 'weight_ih_l' in key or 'weight_hh_l' in key:
                        layer_num = key.split('_')[-1]
                        if layer_num.isdigit():
                            layer_nums.add(int(layer_num))
                self.config['lstm_layers'] = max(layer_nums) + 1 if layer_nums else 2
            else:
                self.config['lstm_layers'] = 2  # é»˜è®¤
        else:
            self.config['lstm_layers'] = 0
    
    def infer_training_config(self):
        """åŸºäºæ©ç å¤§å°æ¨æ–­è®­ç»ƒæ—¶çš„é…ç½®"""
        if 'mask_size' not in self.config or 'block_size' not in self.config:
            return
        
        mask_size = self.config['mask_size']
        block_size = self.config['block_size']
        extra_size = mask_size - block_size
        
        # æ ¹æ®æ©ç å…¬å¼: mask_size = block_size + int(bool(num_props)) + scaffold_maxlen
        prop_contribution = 1 if self.config['has_props'] else 0
        inferred_scaffold_maxlen = extra_size - prop_contribution
        
        # éªŒè¯æ¨æ–­çš„åˆç†æ€§
        if inferred_scaffold_maxlen < 0:
            inferred_scaffold_maxlen = 0
        elif inferred_scaffold_maxlen > 200:  # ä¸åˆç†çš„å¤§å€¼
            if self.config['vocab_size'] == 94:  # GuacaMol
                inferred_scaffold_maxlen = 100  # æ ¹æ®ä¹‹å‰çš„åˆ†æ
            else:
                inferred_scaffold_maxlen = 48  # Mosesé»˜è®¤
        
        self.config['scaffold_maxlen'] = inferred_scaffold_maxlen
        self.config['uses_scaffold'] = inferred_scaffold_maxlen > 0
        
        # éªŒè¯é…ç½®ä¸€è‡´æ€§
        expected_mask_size = block_size + prop_contribution + inferred_scaffold_maxlen
        self.config['config_consistent'] = (expected_mask_size == mask_size)
    
    def detect_dataset(self):
        """æ¨æ–­æ•°æ®é›†ç±»å‹"""
        if 'vocab_size' in self.config and 'block_size' in self.config:
            vocab_size = self.config['vocab_size']
            block_size = self.config['block_size']
            
            # æ ¹æ®è¯æ±‡è¡¨å¤§å°å’Œåºåˆ—é•¿åº¦æ¨æ–­æ•°æ®é›†
            if vocab_size == 94 and block_size == 100:
                self.config['dataset'] = 'guacamol2'
            elif vocab_size == 94 and block_size == 54:
                self.config['dataset'] = 'moses2'  # Mosesä¹Ÿå¯èƒ½ä½¿ç”¨94è¯æ±‡è¡¨
            elif vocab_size == 26:
                self.config['dataset'] = 'moses2'
            else:
                # æ ¹æ®åºåˆ—é•¿åº¦æ¨æ–­
                if block_size >= 90:
                    self.config['dataset'] = 'guacamol2'
                else:
                    self.config['dataset'] = 'moses2'
    
    def analyze(self):
        """å®Œæ•´åˆ†ææ¨¡å‹é…ç½®"""
        print("ğŸ” å¼€å§‹åˆ†ææ¨¡å‹é…ç½®...")
        print("=" * 60)
        
        self.load_checkpoint()
        
        # ğŸ”§ ä¼˜å…ˆä»ä¿å­˜çš„è®­ç»ƒé…ç½®ä¸­è·å–ä¿¡æ¯
        if self.detect_training_config_from_saved():
            print("âœ… æˆåŠŸä»ä¿å­˜çš„è®­ç»ƒé…ç½®ä¸­è·å–å®Œæ•´ä¿¡æ¯")
        else:
            print("âš ï¸  ä½¿ç”¨å¯å‘å¼æ–¹æ³•æ¨æ–­æ¨¡å‹é…ç½®ï¼ˆå¯èƒ½ä¸å¤Ÿå‡†ç¡®ï¼‰")
            self.detect_basic_config()
            self.detect_conditional_features()
            self.infer_training_config()
            self.detect_dataset()
        
        return self.config
    
    def print_config(self):
        """æ‰“å°é…ç½®ä¿¡æ¯"""
        print("\nğŸ“‹ æ£€æµ‹åˆ°çš„æ¨¡å‹é…ç½®:")
        print("-" * 40)
        
        # åŸºæœ¬é…ç½®
        basic_params = ['vocab_size', 'block_size', 'n_layer', 'n_head', 'n_embd', 'mask_size']
        for param in basic_params:
            if param in self.config:
                print(f"  {param}: {self.config[param]}")
        
        print(f"  dataset: {self.config.get('dataset', 'unknown')}")
        
        # æ¡ä»¶ç”Ÿæˆé…ç½®
        print("\nğŸ¯ æ¡ä»¶ç”Ÿæˆé…ç½®:")
        print(f"  has_props: {self.config.get('has_props', False)}")
        print(f"  num_props: {self.config.get('num_props', 0)}")
        print(f"  uses_scaffold: {self.config.get('uses_scaffold', False)}")
        print(f"  scaffold_maxlen: {self.config.get('scaffold_maxlen', 0)}")
        print(f"  has_lstm: {self.config.get('has_lstm', False)}")
        if self.config.get('has_lstm'):
            print(f"  lstm_layers: {self.config.get('lstm_layers', 2)}")
        
        # é…ç½®ä¸€è‡´æ€§
        print(f"\nâœ… é…ç½®ä¸€è‡´æ€§æ£€æŸ¥:")
        consistent = self.config.get('config_consistent', False)
        print(f"  æ©ç å¤§å°åŒ¹é…: {'âœ…' if consistent else 'âŒ'}")
        
        if not consistent:
            print("  âš ï¸  æ£€æµ‹åˆ°é…ç½®ä¸ä¸€è‡´ï¼Œå¯èƒ½æ˜¯è®­ç»ƒè„šæœ¬çš„bugå¯¼è‡´")
        
        # æ©ç åˆ†æ
        if 'mask_size' in self.config and 'block_size' in self.config:
            mask_size = self.config['mask_size']
            block_size = self.config['block_size']
            prop_contrib = 1 if self.config.get('has_props') else 0
            scaffold_len = self.config.get('scaffold_maxlen', 0)
            
            print(f"\nğŸ” æ©ç å¤§å°åˆ†æ:")
            print(f"  æ©ç å¤§å°: {mask_size}")
            print(f"  è®¡ç®—å…¬å¼: {block_size} (åºåˆ—) + {prop_contrib} (å±æ€§) + {scaffold_len} (è„šæ‰‹æ¶) = {block_size + prop_contrib + scaffold_len}")
    
    def generate_commands(self):
        """ç”Ÿæˆä½¿ç”¨å»ºè®®"""
        print(f"\nğŸš€ æ¨èçš„ç”Ÿæˆå‘½ä»¤:")
        print("=" * 50)
        
        # æ¨èä½¿ç”¨ç»Ÿä¸€ç”Ÿæˆè„šæœ¬ï¼ˆè‡ªåŠ¨æ£€æµ‹é…ç½®ï¼‰
        print("ğŸŒŸ æ¨èä½¿ç”¨ç»Ÿä¸€ç”Ÿæˆè„šæœ¬ï¼ˆè‡ªåŠ¨æ£€æµ‹æ¨¡å‹é…ç½®ï¼‰:")
        print("-" * 45)
        
        unified_cmd = [
            "python generate/generate_unified.py",
            f"--model_weight {self.weight_path}",
            "--csv_name your_output_name",
            "--gen_size 1000",
            "--batch_size 32"
        ]
        
        # æ ¹æ®æ£€æµ‹åˆ°çš„é…ç½®æ·»åŠ æ¡ä»¶å‚æ•°
        if self.config.get('has_props'):
            # ğŸ”§ å¦‚æœæœ‰å‡†ç¡®çš„å±æ€§ä¿¡æ¯ï¼Œç›´æ¥ä½¿ç”¨
            if 'props' in self.config and self.config['props']:
                props_str = ' '.join(self.config['props'])
                unified_cmd.append(f"--props {props_str}")
                print(f"  âœ… æ£€æµ‹åˆ°è®­ç»ƒæ—¶ä½¿ç”¨çš„å±æ€§: {props_str}")
            else:
                print("  âš ï¸  æ£€æµ‹åˆ°å±æ€§æ¡ä»¶æ¨¡å‹ï¼Œä½†æ— æ³•ä»æƒé‡æ–‡ä»¶ä¸­ç¡®å®šå…·ä½“å±æ€§ç±»å‹")
                print("  ğŸ“ è¯·æ ¹æ®è®­ç»ƒæ—¶ä½¿ç”¨çš„å±æ€§æ‰‹åŠ¨æŒ‡å®šï¼Œä¾‹å¦‚ï¼š")
                print("      --props qed    (è¯ç‰©ç›¸ä¼¼æ€§)")
                print("      --props sas    (åˆæˆå¯è¾¾æ€§)")  
                print("      --props logp   (è„‚æ°´åˆ†é…ç³»æ•°)")
                print("      --props tpsa   (ææ€§è¡¨é¢ç§¯)")
                unified_cmd.append("--props YOUR_PROPERTY_TYPE")
        
        if self.config.get('uses_scaffold'):
            unified_cmd.append("--scaffold")
        
        if self.config.get('has_lstm'):
            unified_cmd.append("--lstm")
        
        # æ·»åŠ æ•°æ®é›†å‚æ•°
        if 'dataset' in self.config:
            unified_cmd.append(f"--data_name {self.config['dataset']}")
        
        print(" \\\n  ".join(unified_cmd))
        print("\n  âœ¨ ç»Ÿä¸€è„šæœ¬ä¼šè‡ªåŠ¨æ£€æµ‹æ¨¡å‹é…ç½®ï¼Œæ— éœ€æ‰‹åŠ¨æŒ‡å®šæ¶æ„å‚æ•°")
        
        # å¦‚æœç”¨æˆ·éœ€è¦ä½¿ç”¨åŸå§‹ç”Ÿæˆè„šæœ¬ï¼ˆæ‰‹åŠ¨é…ç½®ï¼‰
        if os.path.exists('generate/generate.py'):
            print(f"\nğŸ’¡ å¦‚æœéœ€è¦ä½¿ç”¨åŸå§‹ç”Ÿæˆè„šæœ¬ï¼ˆæ‰‹åŠ¨é…ç½®ï¼‰:")
            print("-" * 40)
            
            manual_cmd = ["python generate/generate.py"]
            manual_cmd.append(f"--model_weight {self.weight_path}")
            manual_cmd.append("--csv_name your_output_name")
            
            # æ•°æ®é›†
            if 'dataset' in self.config:
                manual_cmd.append(f"--data_name {self.config['dataset']}")
            
            # åŸºæœ¬å‚æ•°ï¼ˆéœ€è¦æ‰‹åŠ¨æŒ‡å®šï¼‰
            basic_params = ['vocab_size', 'block_size', 'n_layer', 'n_head', 'n_embd']
            for param in basic_params:
                if param in self.config:
                    manual_cmd.append(f"--{param} {self.config[param]}")
            
            # æ¡ä»¶å‚æ•°
            if self.config.get('has_props'):
                if 'props' in self.config and self.config['props']:
                    props_str = ' '.join(self.config['props'])
                    manual_cmd.append(f"--props {props_str}")
                else:
                    manual_cmd.append("--props YOUR_PROPERTY_TYPE")
            
            if self.config.get('uses_scaffold'):
                manual_cmd.append("--scaffold")
            
            if self.config.get('has_lstm'):
                manual_cmd.append("--lstm")
                manual_cmd.append(f"--lstm_layers {self.config.get('lstm_layers', 2)}")
            
            # ç”Ÿæˆå‚æ•°
            manual_cmd.append("--gen_size 1000")
            manual_cmd.append("--batch_size 32")
            
            print(" \\\n  ".join(manual_cmd))
            print("\n  âš ï¸  åŸå§‹è„šæœ¬éœ€è¦æ‰‹åŠ¨æŒ‡å®šæ‰€æœ‰æ¶æ„å‚æ•°")
            
        # æ·»åŠ å±æ€§ç±»å‹è¯´æ˜
        if self.config.get('has_props'):
            print(f"\nğŸ“‹ å¸¸ç”¨å±æ€§ç±»å‹è¯´æ˜:")
            print("-" * 30)
            print("  qed  : è¯ç‰©ç›¸ä¼¼æ€§ (Drug-likeness)")
            print("  sas  : åˆæˆå¯è¾¾æ€§ (Synthetic Accessibility)")
            print("  logp : è„‚æ°´åˆ†é…ç³»æ•° (Lipophilicity)")
            print("  tpsa : ææ€§è¡¨é¢ç§¯ (Topological Polar Surface Area)")
            print("\n  ğŸ’¡ è¯·æ ¹æ®æ‚¨çš„è®­ç»ƒå‘½ä»¤ä¸­ä½¿ç”¨çš„ --props å‚æ•°æ¥é€‰æ‹©")

def main():
    parser = argparse.ArgumentParser(description='ç»Ÿä¸€æ¨¡å‹é…ç½®æ£€æµ‹å·¥å…·')
    parser.add_argument('weight_path', type=str, help='æƒé‡æ–‡ä»¶è·¯å¾„(.pt)')
    parser.add_argument('--json', action='store_true', help='è¾“å‡ºJSONæ ¼å¼çš„é…ç½®')
    parser.add_argument('--verbose', action='store_true', help='æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯')
    
    args = parser.parse_args()
    
    try:
        detector = ModelConfigDetector(args.weight_path)
        config = detector.analyze()
        
        detector.print_config()
        detector.generate_commands()
        
        if args.json:
            print(f"\nğŸ“„ JSONé…ç½®è¾“å‡º:")
            print(json.dumps(config, indent=2, ensure_ascii=False))
        
        if args.verbose:
            print(f"\nğŸ” è¯¦ç»†æƒé‡ä¿¡æ¯:")
            print("-" * 40)
            for key, value in detector.checkpoint.items():
                shape_str = str(value.shape) if hasattr(value, 'shape') else str(type(value))
                print(f"  {key:<40} {shape_str}")
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {str(e)}")
        return 1
    
    return 0

if __name__ == '__main__':
    exit(main()) 