#!/usr/bin/env python3
"""
ç»Ÿä¸€æ¨¡å‹é…ç½®æ£€æµ‹è„šæœ¬
ç²¾ç¡®æ£€æµ‹PyTorchæ¨¡å‹æƒé‡æ–‡ä»¶(.pt)çš„è®­ç»ƒé…ç½®å¹¶æä¾›ç”Ÿæˆå‘½ä»¤å»ºè®®
"""

import torch
import argparse
import os
import json
from collections import OrderedDict

class ModelConfigDetector:
    """æ¨¡å‹é…ç½®æ£€æµ‹å™¨"""
    
    def __init__(self, weight_path):
        self.weight_path = weight_path
        self.checkpoint = None
        self.config = {}
        self.training_config = {}
        self.new_checkpoint_format = False
        
    def load_checkpoint(self):
        """åŠ è½½æƒé‡æ–‡ä»¶"""
        if not os.path.exists(self.weight_path):
            raise FileNotFoundError(f"æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {self.weight_path}")
        
        try:
            checkpoint_data = torch.load(self.weight_path, map_location='cpu', weights_only=False)
            print(f"âœ… æˆåŠŸåŠ è½½æƒé‡æ–‡ä»¶: {self.weight_path}")
            print(f"ğŸ“ æ–‡ä»¶å¤§å°: {os.path.getsize(self.weight_path) / (1024*1024):.2f} MB")
            
            # ğŸ”§ æ£€æŸ¥æ˜¯å¦ä¸ºæ–°æ ¼å¼ï¼ˆåŒ…å«è®­ç»ƒé…ç½®ï¼‰
            if isinstance(checkpoint_data, dict) and 'model_state_dict' in checkpoint_data:
                print("ğŸ†• æ£€æµ‹åˆ°æ–°æ ¼å¼æƒé‡æ–‡ä»¶ï¼ˆåŒ…å«è®­ç»ƒé…ç½®ï¼‰")
                self.checkpoint = checkpoint_data['model_state_dict']
                self.training_config = checkpoint_data.get('training_config', {}) or {}
                self.new_checkpoint_format = True
                print(f"ğŸ“‹ è®­ç»ƒé…ç½®ä¿¡æ¯: {self.training_config}")
            else:
                print("ğŸ”„ æ£€æµ‹åˆ°æ—§æ ¼å¼æƒé‡æ–‡ä»¶ï¼ˆä»…åŒ…å«æ¨¡å‹æƒé‡ï¼‰")
                self.checkpoint = checkpoint_data
                self.training_config = {}
                self.new_checkpoint_format = False
            
            print(f"ğŸ”¢ å‚æ•°æ€»æ•°: {len(self.checkpoint)}")
        except Exception as e:
            raise RuntimeError(f"æ— æ³•åŠ è½½æƒé‡æ–‡ä»¶: {str(e)}")
    
    def detect_training_config_from_saved(self):
        """ä»ä¿å­˜çš„è®­ç»ƒé…ç½®ä¸­è·å–ä¿¡æ¯"""
        if self.training_config:
            for key, value in self.training_config.items():
                self.config[key] = value

            self.config['has_props'] = self.config.get('num_props', 0) > 0
            self.config['uses_scaffold'] = self.config.get('scaffold', False)
            self.config['has_lstm'] = self.config.get('lstm', False)
            self.config['has_atom_cond'] = self.config.get('atom_cond', False)
            self.config['atom_list'] = self.config.get('atom_list', []) or []
            self.config['props'] = self.config.get('props', []) or []
            if self.config.get('has_atom_cond'):
                self.config['atom_vocab_size'] = self.config.get('atom_vocab_size', len(self.config['atom_list']))
            else:
                self.config['atom_vocab_size'] = 0

            return True
        return False
    
    def detect_basic_config(self):
        """æ£€æµ‹åŸºæœ¬é…ç½®å‚æ•°"""
        # è¯æ±‡è¡¨å¤§å°å’ŒåµŒå…¥ç»´åº¦
        if 'tok_emb.weight' in self.checkpoint:
            self.config['vocab_size'] = self.checkpoint['tok_emb.weight'].shape[0]
            self.config['n_embd'] = self.checkpoint['tok_emb.weight'].shape[1]
        
        # åºåˆ—é•¿åº¦
        if 'pos_emb' in self.checkpoint:
            self.config['block_size'] = self.checkpoint['pos_emb'].shape[1]
        
        # å±‚æ•°
        layer_keys = [k for k in self.checkpoint.keys() if k.startswith('blocks.')]
        if layer_keys:
            layer_nums = set()
            for key in layer_keys:
                parts = key.split('.')
                if len(parts) >= 2 and parts[1].isdigit():
                    layer_nums.add(int(parts[1]))
            self.config['n_layer'] = max(layer_nums) + 1 if layer_nums else 0
        
        # æ³¨æ„åŠ›å¤´æ•°æ¨æ–­
        if 'n_embd' in self.config:
            # å¸¸è§çš„å¤´æ•°é…ç½®
            possible_heads = [1, 2, 4, 6, 8, 12, 16, 20, 24, 32]
            for heads in possible_heads:
                if self.config['n_embd'] % heads == 0:
                    self.config['n_head'] = heads
                    break
            if 'n_head' not in self.config:
                self.config['n_head'] = 8  # é»˜è®¤å€¼
        
        # æ³¨æ„åŠ›æ©ç å¤§å°
        mask_keys = [k for k in self.checkpoint.keys() if 'attn.mask' in k]
        if mask_keys:
            self.config['mask_size'] = self.checkpoint[mask_keys[0]].shape[-1]
    
    def detect_conditional_features(self):
        """æ£€æµ‹æ¡ä»¶ç”Ÿæˆç‰¹å¾"""
        # æ£€æµ‹å±æ€§å±‚
        prop_keys = [k for k in self.checkpoint.keys() if 'prop_nn' in k]
        self.config['has_props'] = len(prop_keys) > 0
        
        if self.config['has_props']:
            if 'prop_nn.weight' in self.checkpoint:
                self.config['num_props'] = self.checkpoint['prop_nn.weight'].shape[1]
            else:
                self.config['num_props'] = 1  # é»˜è®¤
        else:
            self.config['num_props'] = 0
        
        # æ£€æµ‹è„šæ‰‹æ¶å±‚
        scaffold_keys = [k for k in self.checkpoint.keys() if 'scaffold' in k.lower()]
        self.config['has_scaffold_layers'] = len(scaffold_keys) > 0

        atom_keys = [k for k in self.checkpoint.keys() if k.startswith('atom_nn')]
        self.config['has_atom_cond'] = len(atom_keys) > 0
        if self.config['has_atom_cond'] and 'atom_nn.weight' in self.checkpoint:
            self.config['atom_vocab_size'] = self.checkpoint['atom_nn.weight'].shape[1]
        else:
            self.config['atom_vocab_size'] = self.config.get('atom_vocab_size', 0)
        self.config.setdefault('atom_list', [])
        self.config.setdefault('props', [])

        # æ£€æµ‹LSTMå±‚
        lstm_keys = [k for k in self.checkpoint.keys() if 'lstm' in k.lower()]
        self.config['has_lstm'] = len(lstm_keys) > 0
        
        if self.config['has_lstm']:
            # æ¨æ–­LSTMå±‚æ•°
            lstm_layer_keys = [k for k in lstm_keys if 'weight_ih_l' in k or 'weight_hh_l' in k]
            if lstm_layer_keys:
                layer_nums = set()
                for key in lstm_layer_keys:
                    if 'weight_ih_l' in key or 'weight_hh_l' in key:
                        layer_num = key.split('_')[-1]
                        if layer_num.isdigit():
                            layer_nums.add(int(layer_num))
                self.config['lstm_layers'] = max(layer_nums) + 1 if layer_nums else 2
            else:
                self.config['lstm_layers'] = 2  # é»˜è®¤
        else:
            self.config['lstm_layers'] = 0
    
    def infer_training_config(self):
        """åŸºäºæ©ç å¤§å°æ¨æ–­è®­ç»ƒæ—¶çš„é…ç½®"""
        if 'mask_size' not in self.config or 'block_size' not in self.config:
            return
        
        mask_size = self.config['mask_size']
        block_size = self.config['block_size']
        extra_size = mask_size - block_size
        
        prop_contribution = 1 if self.config['has_props'] else 0
        atom_contribution = 1 if self.config.get('has_atom_cond') else 0
        inferred_scaffold_maxlen = extra_size - prop_contribution - atom_contribution
        
        # éªŒè¯æ¨æ–­çš„åˆç†æ€§
        if inferred_scaffold_maxlen < 0:
            inferred_scaffold_maxlen = 0
        elif inferred_scaffold_maxlen > 200:  # ä¸åˆç†çš„å¤§å€¼
            if self.config['vocab_size'] == 94:  # GuacaMol
                inferred_scaffold_maxlen = 100  # æ ¹æ®ä¹‹å‰çš„åˆ†æ
            else:
                inferred_scaffold_maxlen = 48  # Mosesé»˜è®¤
        
        self.config['scaffold_maxlen'] = inferred_scaffold_maxlen
        self.config['uses_scaffold'] = inferred_scaffold_maxlen > 0
        
        # éªŒè¯é…ç½®ä¸€è‡´æ€§
        expected_mask_size = block_size + prop_contribution + atom_contribution + inferred_scaffold_maxlen
        self.config['config_consistent'] = (expected_mask_size == mask_size)
    
    def detect_dataset(self):
        """æ¨æ–­æ•°æ®é›†ç±»å‹"""
        if 'vocab_size' in self.config and 'block_size' in self.config:
            vocab_size = self.config['vocab_size']
            block_size = self.config['block_size']
            
            # æ ¹æ®è¯æ±‡è¡¨å¤§å°å’Œåºåˆ—é•¿åº¦æ¨æ–­æ•°æ®é›†
            if vocab_size == 94 and block_size == 100:
                self.config['dataset'] = 'guacamol2'
            elif vocab_size == 94 and block_size == 54:
                self.config['dataset'] = 'moses2'  # Mosesä¹Ÿå¯èƒ½ä½¿ç”¨94è¯æ±‡è¡¨
            elif vocab_size == 26:
                self.config['dataset'] = 'moses2'
            else:
                # æ ¹æ®åºåˆ—é•¿åº¦æ¨æ–­
                if block_size >= 90:
                    self.config['dataset'] = 'guacamol2'
                else:
                    self.config['dataset'] = 'moses2'
    
    def analyze(self):
        """å®Œæ•´åˆ†ææ¨¡å‹é…ç½®"""
        print("ğŸ” å¼€å§‹åˆ†ææ¨¡å‹é…ç½®...")
        print("=" * 60)
        
        self.load_checkpoint()
        
        # ğŸ”§ ä¼˜å…ˆä»ä¿å­˜çš„è®­ç»ƒé…ç½®ä¸­è·å–ä¿¡æ¯
        if self.detect_training_config_from_saved():
            print("âœ… æˆåŠŸä»ä¿å­˜çš„è®­ç»ƒé…ç½®ä¸­è·å–å®Œæ•´ä¿¡æ¯")
        else:
            print("âš ï¸  ä½¿ç”¨å¯å‘å¼æ–¹æ³•æ¨æ–­æ¨¡å‹é…ç½®ï¼ˆå¯èƒ½ä¸å¤Ÿå‡†ç¡®ï¼‰")
            self.detect_basic_config()
            self.detect_conditional_features()
            self.infer_training_config()
            self.detect_dataset()

        self.config.setdefault('props', [])
        self.config.setdefault('atom_list', [])
        
        return self.config
    
    def print_config(self):
        """æ‰“å°é…ç½®ä¿¡æ¯"""
        print("\nğŸ“‹ æ£€æµ‹åˆ°çš„æ¨¡å‹é…ç½®:")
        print("-" * 40)
        
        # åŸºæœ¬é…ç½®
        basic_params = ['vocab_size', 'block_size', 'n_layer', 'n_head', 'n_embd', 'mask_size']
        for param in basic_params:
            if param in self.config:
                print(f"  {param}: {self.config[param]}")
        
        print(f"  dataset: {self.config.get('dataset', 'unknown')}")
        
        # æ¡ä»¶ç”Ÿæˆé…ç½®
        print("\nğŸ¯ æ¡ä»¶ç”Ÿæˆé…ç½®:")
        print(f"  has_props: {self.config.get('has_props', False)}")
        print(f"  num_props: {self.config.get('num_props', 0)}")
        print(f"  uses_scaffold: {self.config.get('uses_scaffold', False)}")
        print(f"  scaffold_maxlen: {self.config.get('scaffold_maxlen', 0)}")
        print(f"  has_lstm: {self.config.get('has_lstm', False)}")
        if self.config.get('has_lstm'):
            print(f"  lstm_layers: {self.config.get('lstm_layers', 2)}")
        print(f"  has_atom_cond: {self.config.get('has_atom_cond', False)}")
        if self.config.get('has_atom_cond'):
            print(f"  atom_vocab_size: {self.config.get('atom_vocab_size', 0)}")
            if self.config.get('atom_list'):
                print(f"  atom_list: {', '.join(self.config['atom_list'])}")
        
        # é…ç½®ä¸€è‡´æ€§
        print(f"\nâœ… é…ç½®ä¸€è‡´æ€§æ£€æŸ¥:")
        consistent = self.config.get('config_consistent', False)
        print(f"  æ©ç å¤§å°åŒ¹é…: {'âœ…' if consistent else 'âŒ'}")
        
        if not consistent:
            print("  âš ï¸  æ£€æµ‹åˆ°é…ç½®ä¸ä¸€è‡´ï¼Œå¯èƒ½æ˜¯è®­ç»ƒè„šæœ¬çš„bugå¯¼è‡´")
        
        # æ©ç åˆ†æ
        if 'mask_size' in self.config and 'block_size' in self.config:
            mask_size = self.config['mask_size']
            block_size = self.config['block_size']
            prop_contrib = 1 if self.config.get('has_props') else 0
            atom_contrib = 1 if self.config.get('has_atom_cond') else 0
            scaffold_len = self.config.get('scaffold_maxlen', 0)
            
            print(f"\nğŸ” æ©ç å¤§å°åˆ†æ:")
            print(f"  æ©ç å¤§å°: {mask_size}")
            print(f"  è®¡ç®—å…¬å¼: {block_size} (åºåˆ—) + {prop_contrib} (å±æ€§) + {atom_contrib} (åŸå­) + {scaffold_len} (è„šæ‰‹æ¶) = {block_size + prop_contrib + atom_contrib + scaffold_len}")
    
    def generate_commands(self):
        """ç”Ÿæˆä½¿ç”¨å»ºè®®"""
        print(f"\nğŸš€ æ¨èçš„ç”Ÿæˆå‘½ä»¤:")
        print("=" * 50)
        
        # æ¨èä½¿ç”¨ç»Ÿä¸€ç”Ÿæˆè„šæœ¬ï¼ˆè‡ªåŠ¨æ£€æµ‹é…ç½®ï¼‰
        csv_name = os.path.splitext(os.path.basename(self.weight_path))[0]
        data_name = self.config.get('data_name') or self.config.get('dataset') or 'moses2'
        has_atom_cond = self.config.get('has_atom_cond', False)
        atom_list = [atom for atom in self.config.get('atom_list', []) if atom]
        missing_props = self.config.get('has_props', False) and not self.config.get('props')
        
        if not has_atom_cond:
            print("ğŸŒŸ æ¨èä½¿ç”¨ç»Ÿä¸€ç”Ÿæˆè„šæœ¬ï¼ˆè‡ªåŠ¨æ£€æµ‹æ¨¡å‹é…ç½®ï¼‰:")
            print("-" * 45)
            unified_cmd = [
                "python generate/generate_unified.py",
                f"--model_weight {self.weight_path}",
                f"--csv_name {csv_name}",
                "--gen_size 10000",
                "--batch_size 32"
            ]
            
            if data_name:
                unified_cmd.append(f"--data_name {data_name}")
            
            if self.config.get('has_props'):
                if self.config.get('props'):
                    props_str = ' '.join(self.config['props'])
                    unified_cmd.append(f"--props {props_str}")
                    print(f"  âœ… æ£€æµ‹åˆ°è®­ç»ƒæ—¶ä½¿ç”¨çš„å±æ€§: {props_str}")
                else:
                    unified_cmd.append("--props YOUR_PROPERTY_TYPE")
                    print("  âš ï¸  æ£€æµ‹åˆ°å±æ€§æ¡ä»¶æ¨¡å‹ï¼Œä½†æ— æ³•ä»æƒé‡æ–‡ä»¶ä¸­ç¡®å®šå…·ä½“å±æ€§ç±»å‹")
                    print("  ğŸ“ ç¤ºä¾‹ï¼š--props qed | --props sas | --props logp | --props tpsa")
            if self.config.get('uses_scaffold'):
                unified_cmd.append("--scaffold")
            if self.config.get('has_lstm'):
                unified_cmd.append("--lstm")
            
            print(" \\\n  ".join(unified_cmd))
            print("\n  âœ¨ ç»Ÿä¸€è„šæœ¬ä¼šè‡ªåŠ¨æ£€æµ‹æ¨¡å‹é…ç½®ï¼Œæ— éœ€æ‰‹åŠ¨æŒ‡å®šæ¶æ„å‚æ•°")
        else:
            print("ğŸŒŸ æ¨èä½¿ç”¨ç»Ÿä¸€ç”Ÿæˆè„šæœ¬ï¼ˆè‡ªåŠ¨æ£€æµ‹æ¨¡å‹é…ç½®ï¼‰:")
            print("-" * 45)
            print("  âš ï¸  å½“å‰ç»Ÿä¸€è„šæœ¬å°šæœªæ”¯æŒåŸå­æ¡ä»¶ï¼Œè¯·ä½¿ç”¨ä¸‹æ–¹çš„æ‰‹åŠ¨å‘½ä»¤")
        
        # æ‰‹åŠ¨è„šæœ¬æ¨èé€»è¾‘
        generate_new = os.path.exists('fangpt_generate/generate.py')
        generate_old = os.path.exists('generate/generate.py')
        needs_new_script = has_atom_cond or self.new_checkpoint_format
        
        preferred_script = None
        fallback_warning = None
        if needs_new_script:
            if generate_new:
                preferred_script = 'fangpt_generate/generate.py'
            elif generate_old:
                preferred_script = 'generate/generate.py'
                fallback_warning = "âš ï¸  è­¦å‘Š: æ—§ç‰ˆè„šæœ¬å¯èƒ½æ— æ³•æ­£ç¡®åŠ è½½æ–°æ ¼å¼æˆ–åŸå­æ¡ä»¶æ¨¡å‹ï¼Œè¯·è°¨æ…ä½¿ç”¨"
        else:
            if generate_old:
                preferred_script = 'generate/generate.py'
            elif generate_new:
                preferred_script = 'fangpt_generate/generate.py'
        
        if not preferred_script:
            print("\nâŒ æœªæ‰¾åˆ°å¯ç”¨çš„ç”Ÿæˆè„šæœ¬ï¼Œè¯·ç¡®è®¤ä»“åº“ä¸­å­˜åœ¨ generate/ æˆ– fangpt_generate/ ç›®å½•ã€‚")
            return
        
        print(f"\nğŸ’¡ æ‰‹åŠ¨ç”Ÿæˆè„šæœ¬å»ºè®®:")
        print("-" * 40)
        if preferred_script == 'fangpt_generate/generate.py':
            print("  âœ… å°†ä½¿ç”¨æ–°ç‰ˆç”Ÿæˆè„šæœ¬ fangpt_generate/generate.py")
        else:
            print("  âœ… å°†ä½¿ç”¨æ—§ç‰ˆç”Ÿæˆè„šæœ¬ generate/generate.py")
            if self.new_checkpoint_format:
                print("  âš ï¸ æ£€æµ‹åˆ°æ–°æ ¼å¼æƒé‡ï¼Œæ—§ç‰ˆè„šæœ¬å¯èƒ½éœ€è¦æ‰‹åŠ¨ä¿®æ”¹åŠ è½½é€»è¾‘")
        
        manual_cmd = [
            f"python {preferred_script}",
            f"--model_weight {self.weight_path}",
            f"--csv_name {csv_name}",
            "--gen_size 10000",
            "--batch_size 32"
        ]
        
        if data_name:
            manual_cmd.append(f"--data_name {data_name}")
        
        if preferred_script == 'generate/generate.py':
            basic_params = ['vocab_size', 'block_size', 'n_layer', 'n_head', 'n_embd']
            for param in basic_params:
                if param in self.config:
                    manual_cmd.append(f"--{param} {self.config[param]}")
        
        if self.config.get('uses_scaffold'):
            manual_cmd.append("--scaffold")
        if self.config.get('has_lstm'):
            manual_cmd.append("--lstm")
            manual_cmd.append(f"--lstm_layers {self.config.get('lstm_layers', 2)}")
        
        if self.config.get('has_props'):
            if self.config.get('props'):
                props_str = ' '.join(self.config['props'])
                manual_cmd.append(f"--props {props_str}")
            else:
                manual_cmd.append("--props YOUR_PROPERTY_TYPE")
        
        if has_atom_cond and atom_list:
            atom_str = ' '.join(atom_list)
            manual_cmd.append(f"--atom_list {atom_str}")
        elif has_atom_cond:
            print("  âš ï¸ æ£€æµ‹åˆ°åŸå­æ¡ä»¶ï¼Œä½†æœªèƒ½è§£æå‡ºè®­ç»ƒæ—¶çš„ atom_listï¼Œè¯·æ‰‹åŠ¨è¡¥å……")
        
        print(" \\\n  ".join(manual_cmd))
        if fallback_warning:
            print(f"\n  {fallback_warning}")
        elif preferred_script == 'generate/generate.py':
            print("\n  âš ï¸  åŸå§‹è„šæœ¬éœ€è¦æ‰‹åŠ¨æŒ‡å®šæ‰€æœ‰æ¶æ„å‚æ•°")
        if missing_props:
            print("  ğŸ“ è¯·æ ¹æ®è®­ç»ƒæ—¶çš„å±æ€§è®¾ç½®æ›¿æ¢ --props YOUR_PROPERTY_TYPE")
        
        if has_atom_cond:
            if atom_list:
                print(f"  ğŸ”¬ æ£€æµ‹åˆ°åŸå­æ¡ä»¶: {', '.join(atom_list)}")
            print("  ğŸ‘‰ è¯·æ ¹æ®ç›®æ ‡åŸå­æˆ–å‘é‡è¡¥å…… --atom_conditionï¼Œä¾‹å¦‚ï¼š")
            print("      --atom_condition O")
            print("      --atom_condition 1 0 0")
        
        # æ·»åŠ å±æ€§ç±»å‹è¯´æ˜
        if self.config.get('has_props'):
            print(f"\nğŸ“‹ å¸¸ç”¨å±æ€§ç±»å‹è¯´æ˜:")
            print("-" * 30)
            print("  qed  : è¯ç‰©ç›¸ä¼¼æ€§ (Drug-likeness)")
            print("  sas  : åˆæˆå¯è¾¾æ€§ (Synthetic Accessibility)")
            print("  logp : è„‚æ°´åˆ†é…ç³»æ•° (Lipophilicity)")
            print("  tpsa : ææ€§è¡¨é¢ç§¯ (Topological Polar Surface Area)")
            print("\n  ğŸ’¡ è¯·æ ¹æ®æ‚¨çš„è®­ç»ƒå‘½ä»¤ä¸­ä½¿ç”¨çš„ --props å‚æ•°æ¥é€‰æ‹©")

def main():
    parser = argparse.ArgumentParser(description='ç»Ÿä¸€æ¨¡å‹é…ç½®æ£€æµ‹å·¥å…·')
    parser.add_argument('weight_path', type=str, help='æƒé‡æ–‡ä»¶è·¯å¾„(.pt)')
    parser.add_argument('--json', action='store_true', help='è¾“å‡ºJSONæ ¼å¼çš„é…ç½®')
    parser.add_argument('--verbose', action='store_true', help='æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯')
    
    args = parser.parse_args()
    
    try:
        detector = ModelConfigDetector(args.weight_path)
        config = detector.analyze()
        
        detector.print_config()
        detector.generate_commands()
        
        if args.json:
            print(f"\nğŸ“„ JSONé…ç½®è¾“å‡º:")
            print(json.dumps(config, indent=2, ensure_ascii=False))
        
        if args.verbose:
            print(f"\nğŸ” è¯¦ç»†æƒé‡ä¿¡æ¯:")
            print("-" * 40)
            for key, value in detector.checkpoint.items():
                shape_str = str(value.shape) if hasattr(value, 'shape') else str(type(value))
                print(f"  {key:<40} {shape_str}")
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {str(e)}")
        return 1
    
    return 0

if __name__ == '__main__':
    exit(main()) 
