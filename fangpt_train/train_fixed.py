import pandas as pd
import argparse
from utils import set_seed
import numpy as np
import wandb

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.nn import functional as F
from torch.cuda.amp import GradScaler

from model import GPT, GPTConfig
from trainer import Trainer, TrainerConfig
from dataset import SmileDataset
import math
from utils import SmilesEnumerator
import re

if __name__ == '__main__':

    parser = argparse.ArgumentParser()

    parser.add_argument('--run_name', type=str,
                        help="name for wandb run", required=False)
    parser.add_argument('--debug', action='store_true',
                        default=False, help='debug')
    # in moses dataset, on average, there are only 5 molecules per scaffold
    parser.add_argument('--scaffold', action='store_true',
                        default=False, help='condition on scaffold')
    parser.add_argument('--lstm', action='store_true',
                        default=False, help='use lstm for transforming scaffold')
    parser.add_argument('--data_name', type=str, default='moses2',
                        help="name of the dataset to train on", required=False)
    # parser.add_argument('--property', type=str, default = 'qed', help="which property to use for condition", required=False)
    parser.add_argument('--props', nargs="+", default=['qed'],
                        help="properties to be used for condition", required=False)
    parser.add_argument('--num_props', type=int, default = 0, help="number of properties to use for condition", required=False)
    # parser.add_argument('--prop1_unique', type=int, default = 0, help="unique values in that property", required=False)
    parser.add_argument('--n_layer', type=int, default=8,
                        help="number of layers", required=False)
    parser.add_argument('--n_head', type=int, default=8,
                        help="number of heads", required=False)
    parser.add_argument('--n_embd', type=int, default=256,
                        help="embedding dimension", required=False)
    parser.add_argument('--max_epochs', type=int, default=10,
                        help="total epochs", required=False)
    parser.add_argument('--batch_size', type=int, default=512,
                        help="batch size", required=False)
    parser.add_argument('--learning_rate', type=int,
                        default=6e-4, help="learning rate", required=False)
    parser.add_argument('--lstm_layers', type=int, default=0,
                        help="number of layers in lstm", required=False)

    args = parser.parse_args()

    set_seed(42)

    wandb.init(project="lig_gpt", name=args.run_name)

    data = pd.read_csv('datasets/' + args.data_name + '.csv')
    data = data.dropna(axis=0).reset_index(drop=True)
    # data = data.sample(frac = 0.1).reset_index(drop=True)
    data.columns = data.columns.str.lower()

    # ğŸ”§ ä¼˜åŒ–ï¼šç»Ÿä¸€å¤„ç†ä¸åŒæ•°æ®é›†çš„åˆ—åå·®å¼‚
    if 'moses' in args.data_name:
        # Mosesæ•°æ®é›†ä½¿ç”¨'split'åˆ—
        train_data = data[data['split'] == 'train'].reset_index(drop=True)
        val_data = data[data['split'] == 'test'].reset_index(drop=True)  # Mosesç”¨testä½œä¸ºéªŒè¯é›†
    else:
        # GuacaMolæ•°æ®é›†ä½¿ç”¨'source'åˆ—
        train_data = data[data['source'] == 'train'].reset_index(drop=True)
        val_data = data[data['source'] == 'val'].reset_index(drop=True)   # GuacaMolç”¨valä½œä¸ºéªŒè¯é›†

    # train_data = train_data.sample(frac = 0.1, random_state = 42).reset_index(drop=True)
    # val_data = val_data.sample(frac = 0.1, random_state = 42).reset_index(drop=True)

    smiles = train_data['smiles']
    vsmiles = val_data['smiles']

    # prop = train_data[['qed']]
    # vprop = val_data[['qed']]

    prop = train_data[args.props].values.tolist()
    vprop = val_data[args.props].values.tolist()
    num_props = args.num_props

    # ğŸ”§ ä¿®å¤ï¼šåªæœ‰åœ¨å¯ç”¨scaffoldæ—¶æ‰å¤„ç†è„šæ‰‹æ¶æ•°æ®
    if args.scaffold:
        print("å¯ç”¨è„šæ‰‹æ¶æ¡ä»¶ç”Ÿæˆï¼Œæ­£åœ¨å¤„ç†è„šæ‰‹æ¶æ•°æ®...")
        scaffold = train_data['scaffold_smiles']
        vscaffold = val_data['scaffold_smiles']
        
        pattern = "(\[[^\]]+]|<|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\(|\)|\.|=|#|-|\+|\\\\|\/|:|~|@|\?|>|\*|\$|\%[0-9]{2}|[0-9])"
        regex = re.compile(pattern)

        # è®¡ç®—åˆ†å­åºåˆ—çš„æœ€å¤§é•¿åº¦
        lens = [len(regex.findall(i.strip()))
                  for i in (list(smiles.values) + list(vsmiles.values))]
        max_len = max(lens)
        print('Max len: ', max_len)

        # è®¡ç®—è„šæ‰‹æ¶çš„æœ€å¤§é•¿åº¦
        lens = [len(regex.findall(i.strip()))
                for i in (list(scaffold.values) + list(vscaffold.values))]
        scaffold_max_len = max(lens)
        print('Scaffold max len: ', scaffold_max_len)

        # å¯¹åˆ†å­åºåˆ—è¿›è¡Œpadding
        smiles = [i + str('<')*(max_len - len(regex.findall(i.strip())))
                    for i in smiles]
        vsmiles = [i + str('<')*(max_len - len(regex.findall(i.strip())))
                    for i in vsmiles]

        # å¯¹è„šæ‰‹æ¶åºåˆ—è¿›è¡Œpadding
        scaffold = [i + str('<')*(scaffold_max_len -
                                    len(regex.findall(i.strip()))) for i in scaffold]
        vscaffold = [i + str('<')*(scaffold_max_len -
                                    len(regex.findall(i.strip()))) for i in vscaffold]
        
        print(f"è„šæ‰‹æ¶æ¡ä»¶ç”Ÿæˆé…ç½®ï¼šscaffold_maxlen={scaffold_max_len}")
        
    else:
        print("æœªå¯ç”¨è„šæ‰‹æ¶æ¡ä»¶ç”Ÿæˆï¼Œä½¿ç”¨æ— è„šæ‰‹æ¶é…ç½®...")
        scaffold = None
        vscaffold = None
        scaffold_max_len = 0
        
        pattern = "(\[[^\]]+]|<|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\(|\)|\.|=|#|-|\+|\\\\|\/|:|~|@|\?|>|\*|\$|\%[0-9]{2}|[0-9])"
        regex = re.compile(pattern)

        # åªè®¡ç®—åˆ†å­åºåˆ—çš„æœ€å¤§é•¿åº¦
        lens = [len(regex.findall(i.strip()))
                  for i in (list(smiles.values) + list(vsmiles.values))]
        max_len = max(lens)
        print('Max len: ', max_len)

        # åªå¯¹åˆ†å­åºåˆ—è¿›è¡Œpadding
        smiles = [i + str('<')*(max_len - len(regex.findall(i.strip())))
                    for i in smiles]
        vsmiles = [i + str('<')*(max_len - len(regex.findall(i.strip())))
                    for i in vsmiles]
        
        print("æ— è„šæ‰‹æ¶é…ç½®ï¼šscaffold_maxlen=0")

    # è¯æ±‡è¡¨ä¿æŒä¸å˜
    whole_string = ['#', '%10', '%11', '%12', '(', ')', '-', '1', '2', '3', '4', '5', '6', '7', '8', '9', '<', '=', 'B', 'Br', 'C', 'Cl', 'F', 'I', 'N', 'O', 'P', 'S', '[B-]', '[BH-]', '[BH2-]', '[BH3-]', '[B]', '[C+]', '[C-]', '[CH+]', '[CH-]', '[CH2+]', '[CH2]', '[CH]', '[F+]', '[H]', '[I+]', '[IH2]', '[IH]', '[N+]', '[N-]', '[NH+]', '[NH-]', '[NH2+]', '[NH3+]', '[N]', '[O+]', '[O-]', '[OH+]', '[O]', '[P+]', '[PH+]', '[PH2+]', '[PH]', '[S+]', '[S-]', '[SH+]', '[SH]', '[Se+]', '[SeH+]', '[SeH]', '[Se]', '[Si-]', '[SiH-]', '[SiH2]', '[SiH]', '[Si]', '[b-]', '[bH-]', '[c+]', '[c-]', '[cH+]', '[cH-]', '[n+]', '[n-]', '[nH+]', '[nH]', '[o+]', '[s+]', '[sH+]', '[se+]', '[se]', 'b', 'c', 'n', 'o', 'p', 's']

    # ğŸ”§ ä¿®å¤ï¼šæ•°æ®é›†åˆ›å»ºæ—¶æ­£ç¡®å¤„ç†è„šæ‰‹æ¶å‚æ•°
    if args.scaffold:
        train_dataset = SmileDataset(args, smiles, whole_string, max_len, 
                                   prop=prop, aug_prob=0, 
                                   scaffold=scaffold, 
                                   scaffold_maxlen=scaffold_max_len)
        valid_dataset = SmileDataset(args, vsmiles, whole_string, max_len, 
                                   prop=vprop, aug_prob=0, 
                                   scaffold=vscaffold, 
                                   scaffold_maxlen=scaffold_max_len)
    else:
        # æ— è„šæ‰‹æ¶æ—¶ï¼Œåˆ›å»ºdummy scaffoldä»¥ä¿æŒæ•°æ®é›†å…¼å®¹æ€§
        dummy_scaffold_train = ['<'] * len(smiles)
        dummy_scaffold_valid = ['<'] * len(vsmiles)
        train_dataset = SmileDataset(args, smiles, whole_string, max_len, 
                                   prop=prop, aug_prob=0, 
                                   scaffold=dummy_scaffold_train, 
                                   scaffold_maxlen=scaffold_max_len)
        valid_dataset = SmileDataset(args, vsmiles, whole_string, max_len, 
                                   prop=vprop, aug_prob=0, 
                                   scaffold=dummy_scaffold_valid, 
                                   scaffold_maxlen=scaffold_max_len)

    # ğŸ”§ ä¿®å¤ï¼šæ¨¡å‹é…ç½®æ—¶æ­£ç¡®è®¾ç½®è„šæ‰‹æ¶å‚æ•°
    mconf = GPTConfig(train_dataset.vocab_size, train_dataset.max_len, 
                     num_props=num_props,  # args.num_props,
                     n_layer=args.n_layer, n_head=args.n_head, n_embd=args.n_embd, 
                     scaffold=args.scaffold,  # ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„scaffoldå‚æ•°
                     scaffold_maxlen=scaffold_max_len,  # åªæœ‰å¯ç”¨scaffoldæ—¶æ‰éé›¶
                     lstm=args.lstm, lstm_layers=args.lstm_layers)
    model = GPT(mconf)

    # ğŸ”§ æ‰“å°é…ç½®ä¿¡æ¯ä»¥ä¾¿éªŒè¯
    print(f"\n=== æ¨¡å‹é…ç½® ===")
    print(f"vocab_size: {train_dataset.vocab_size}")
    print(f"max_len: {train_dataset.max_len}")
    print(f"num_props: {num_props}")
    print(f"scaffold: {args.scaffold}")
    print(f"scaffold_maxlen: {scaffold_max_len}")
    print(f"n_layer: {args.n_layer}")
    print(f"n_head: {args.n_head}")
    print(f"n_embd: {args.n_embd}")
    
    # è®¡ç®—é¢„æœŸçš„æ©ç å¤§å°
    expected_mask_size = train_dataset.max_len + int(bool(num_props)) + scaffold_max_len
    print(f"é¢„æœŸæ©ç å¤§å°: {expected_mask_size}x{expected_mask_size}")
    print(f"æ©ç è®¡ç®—: {train_dataset.max_len} (åºåˆ—) + {int(bool(num_props))} (å±æ€§) + {scaffold_max_len} (è„šæ‰‹æ¶) = {expected_mask_size}")

    tconf = TrainerConfig(max_epochs=args.max_epochs, batch_size=args.batch_size, learning_rate=args.learning_rate,
                            lr_decay=True, warmup_tokens=0.1*len(train_data)*max_len, final_tokens=args.max_epochs*len(train_data)*max_len,
                            num_workers=10, ckpt_path=f'weights/{args.run_name}.pt', block_size=train_dataset.max_len, generate=False)
    
    # ğŸ”§ æ–°å¢ï¼šä¿å­˜è®­ç»ƒé…ç½®ä¿¡æ¯
    training_config = {
        'props': args.props,
        'num_props': num_props,
        'scaffold': args.scaffold,
        'scaffold_maxlen': scaffold_max_len,
        'lstm': args.lstm,
        'lstm_layers': args.lstm_layers,
        'data_name': args.data_name,
        'vocab_size': train_dataset.vocab_size,
        'block_size': train_dataset.max_len,
        'n_layer': args.n_layer,
        'n_head': args.n_head,
        'n_embd': args.n_embd
    }
    
    trainer = Trainer(model, train_dataset, valid_dataset,
                        tconf, train_dataset.stoi, train_dataset.itos, training_config)
    df = trainer.train(wandb)

    if df is not None:
        df.to_csv(f'{args.run_name}.csv', index=False)
    else:
        print(f"Training completed successfully. Model saved to {tconf.ckpt_path}")
    
    print(f"\n=== è®­ç»ƒå®Œæˆ ===")
    print(f"æ¨¡å‹æƒé‡ä¿å­˜åˆ°: {tconf.ckpt_path}")
    if args.scaffold:
        print("âš ï¸  è¿™æ˜¯ä¸€ä¸ªè„šæ‰‹æ¶æ¡ä»¶ç”Ÿæˆæ¨¡å‹")
        print(f"ç”Ÿæˆæ—¶éœ€è¦ä½¿ç”¨: --scaffold --scaffold_maxlen {scaffold_max_len}")
    else:
        print("âœ… è¿™æ˜¯ä¸€ä¸ªæ— æ¡ä»¶/å±æ€§æ¡ä»¶ç”Ÿæˆæ¨¡å‹")
        print("ç”Ÿæˆæ—¶å¯ä»¥ç›´æ¥ä½¿ç”¨ï¼Œæ— éœ€é¢å¤–çš„è„šæ‰‹æ¶å‚æ•°") 